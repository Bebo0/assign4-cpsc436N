{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESA2kDcMmNkJ"
   },
   "source": [
    "##Assignment 4 (part 1)(CPSC 436N): Count-based Sequence labeling  with Viterbi Algorithm\n",
    "\n",
    "In Part1 of this assignment, we are going to use the the text file called __cmpt-hw2-3.txt__ to build a Hidden Markov Model to predict the part of speech tags for the words in a given sentence using the *Viterbi Algorithm* (see textbook sec 8.4.5). In class we learned about the *Forward Algorithm* and Viterbi is a slight variation of that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AD7kg1fkklb"
   },
   "source": [
    "## 1. Loading the dataset (cmpt-hw2-3.txt)\n",
    "\n",
    "In order to ease later steps, we can load and save the cmpt-hw2-3.txt dataset (i.e., our corpus annotated with POSs) in a format different from its origianl one. In the cmpt-hw2-3.txt each word is followed by an underscore and a tag that represents the word’s correct part of speech in that context. For instance:\n",
    "\n",
    "<br>\n",
    "\n",
    "*(1) There_EX are_VBP also_RB plant_NN and_CC gift_NN shops_NNS ._.*\n",
    "\n",
    "*(2) Tosco_NNP said_VBD it_PRP expects_VBZ BP_NNP to_TO shut_VB the_DT plant_NN ._.*\n",
    "\n",
    "<br>\n",
    "\n",
    "We will instead reformat and load the same info as (token, pos_tag) lists. For example:\n",
    "\n",
    "<br>\n",
    "\n",
    "[\n",
    "  [(there, EX), (are, VBP), (also, RB), (plant, NN), (and, CC), (gift, NN), (shop, NNS), (., .)],\n",
    "\n",
    "  [(tosco, NNP), (said, VBD), (it, PRP), (expects, VBZ), (BP, NNP), (to, To), (shut, VB), (the, DT), (plant, NN), (., .)]\n",
    "]\n",
    "\n",
    "<br>\n",
    "\n",
    "**Please note that we convert each token into its lower case.**\n",
    "\n",
    "Also in this step, we  obtain the set of states for the HHM (the POSs)  and set of observation (the tokens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PmreO0EUmLm-"
   },
   "outputs": [],
   "source": [
    "data_path = './cmpt-hw2-3.txt'\n",
    "\n",
    "dataset = []; states = []; observations = []; #initialize dataset, state and observation list.\n",
    "'''\n",
    "https://medium.com/data-science-in-your-pocket/pos-tagging-using-hidden-markov-models-hmm-viterbi-algorithm-in-nlp-mathematics-explained-d43ca89347c4\n",
    "- Tags are hidden states\n",
    "- words are observations\n",
    "- every word has a state (tag) which is \"hidden\"\n",
    "- Given an input as HMM (Transition Matrix, Emission Matrix) and a sequence of observations O = o1, o2, …, oT \n",
    "(Words in sentences of a corpus), find the most probable sequence of states Q = q1q2q3 …qT (POS Tags in our case)\n",
    "\n",
    "https://www.mygreatlearning.com/blog/pos-tagging/\n",
    "Now, what is the probability that the word Ted is a noun, will is a model, spot is a verb and Will is a noun. These sets of \n",
    "probabilities are Emission probabilities and should be high for our tagging to be likely.\n",
    "\n",
    "'''\n",
    "for line in open(data_path): #load data from the text file.\n",
    "  text_pos_list = [];\n",
    "  l = line.strip().split(' ') #split each line in the file by space.\n",
    "  for w in l:\n",
    "    w = w.split('_') #each token and its pos tag are connected by an underscore, here we split it by underscore.\n",
    "    states.append(w[1]) #add pos tag to the initial state list.\n",
    "    observations.append(w[0].lower()) #lowercase the token and add it to the initial observation list.\n",
    "    text_pos_list.append((w[0].lower(), w[1])) #add the (token, pos) pair into the list for the sentence.\n",
    "  dataset.append(text_pos_list) #add the processed sentence list into the dataset list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5phXfOnGueR_"
   },
   "source": [
    "## 2. Further processing the dataset, state and observation list\n",
    "\n",
    "In this step, we need to consider two issue:\n",
    "\n",
    "**Q1(a)** What is the problem with the state lists we just obtained? report the problem and fix it in the code below.\n",
    "\n",
    "\n",
    "**Q1(b)** The first pass over the dataset generates a fixed list of vocabulary tokens. What would happen if the input sentence we want to tag contained unknown tokens, i.e which are not covered in the vocabulary list? \n",
    "The code below is implementing a possible way to adderess this problem. Please add comments to the lines with \"COMMENT NEEDED\". And describe in your words the implemented solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oW-YJkjcrr7R"
   },
   "outputs": [],
   "source": [
    "#SOLUTION Q1(a)\n",
    "\n",
    "#*************\n",
    "\n",
    "unk_list = []; #COMMENT NEEDED Q1(b). An array to keep track of all unknown word/tag combinations\n",
    "\n",
    "for o in tuple(set(observations)):\n",
    "  if observations.count(o) < 2:\n",
    "     unk_list.append(o) # COMMENT NEEDED Q1(b) a word is unknown if it was only seen once \n",
    "\n",
    "for i in range(len(observations)):\n",
    "  if observations[i] in unk_list:\n",
    "     observations[i] = '_unk_' #COMMENT NEEDED Q1(b) if a word was labelled as unknown, change it to a homogenoues form (_unk_)\n",
    "\n",
    "observations = tuple(set(observations)) #COMMENT NEEDED Q1(b) remove duplicate words in observations i.e make it a set\n",
    "states = tuple(set(states))\n",
    "\n",
    "for text_pos_i in range(len(dataset)): #COMMENT NEEDED Q1(b) traverse all sentences and change any unknown words to _unk_\n",
    "  for w_i in range(len(dataset[text_pos_i])):\n",
    "    if dataset[text_pos_i][w_i][0] in unk_list:\n",
    "       dataset[text_pos_i][w_i] = ('_unk_', dataset[text_pos_i][w_i][1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBqKKgjJzWCT"
   },
   "source": [
    "## 3. Computing the Emission Probabilities\n",
    "\n",
    "The emission probability matrix can be formed as a dictionary, **Q2:** please try to understand the structure of this dictionary and complete the following code to obtain the emission probability matrix with a final normalization+smoothing step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "GR8sPEVUxwHm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGo through every tag\\n    Count the total number of times they're seen\\nGo through every tag\\n    Divide the number of times they're seen with a word by the total number of times they're seen\\n\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "def normalize_and_smoothing(d, all_elements):\n",
    "   for e in all_elements:\n",
    "     if e not in d.keys():\n",
    "       d[e] = 0\n",
    "   raw = sum(d.values())+len(d)\n",
    "   factor = float(1)/raw\n",
    "   output = {key:(value+1)*factor for key,value in d.items()}\n",
    "   return output\n",
    "\n",
    "emission_probability = {} #initialize the emission probability matrix.\n",
    "for sent in dataset: #traverse the dataset to obtain the emission probability matrix.\n",
    "  for i in range(len(sent)):\n",
    "    if sent[i][1] not in emission_probability.keys(): #if the state (pos) is not in the key list of emission probability dict yet, intiailize a subdict for it and add the corresponding observation (token).\n",
    "       emission_probability[sent[i][1]] = {}\n",
    "       emission_probability[sent[i][1]][sent[i][0]] = 1\n",
    "    else: #if the state (pos) is in the key list of emission probability dict, update the frequency of the corresponding observation (token).\n",
    "      if sent[i][0] not in emission_probability[sent[i][1]].keys():\n",
    "         emission_probability[sent[i][1]][sent[i][0]] = 1\n",
    "      else:\n",
    "         emission_probability[sent[i][1]][sent[i][0]] += 1\n",
    "            \n",
    "#****** SOLUTION Q2 *******\n",
    "# states_count = defaultdict(int)\n",
    "\n",
    "    \n",
    "      \n",
    "# for state, word_count_dict in emission_probability.items():\n",
    "#     states_count[state] += sum(word_count_dict.values())\n",
    "\n",
    "\n",
    "# for state, word_count_dict in emission_probability.items():\n",
    "#     for word, count in word_count_dict.items():\n",
    "#         emission_probability[state][word] = emission_probability[state][word]/states_count[state]\n",
    "        \n",
    "# for state, word_count_dict in emission_probability.items():\n",
    "#     emission_probability[state] = normalize_and_smoothing(word_count_dict, observations)\n",
    "        \n",
    "# # print(emission_probability)\n",
    "for state, word_count_dict in emission_probability.items():\n",
    "    emission_probability[state] = normalize_and_smoothing(word_count_dict, observations)\n",
    "        \n",
    "# for state, word_count_dict in emission_probability.items():\n",
    "#     print(sorted(word_count_dict.values()))\n",
    "#     print(\"\\n\")\n",
    "# print(emission_probability)\n",
    "            \n",
    "'''\n",
    "Go through every tag\n",
    "    Count the total number of times they're seen\n",
    "Go through every tag\n",
    "    Divide the number of times they're seen with a word by the total number of times they're seen\n",
    "\n",
    "'''\n",
    "\n",
    "#*************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrgHDEf23WOO"
   },
   "source": [
    "## 4. Computing the Start and Transition Probabilities\n",
    "\n",
    "The Transition and Strat probability matrices can be similarly formed as dictionaries,** Q2 cont'** please implement the normalization function also for the transition probabilitie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "qRvHUphUz2G7"
   },
   "outputs": [],
   "source": [
    "transition_probability = {} #initialize the transition probability matrix.\n",
    "\n",
    "for sent in dataset: #traverse the dataset to obtain the transition probability matrix.\n",
    "  for i in range(len(sent)-1):\n",
    "    if sent[i][1] not in transition_probability.keys(): #if the current state (pos) is not in the key list of transition probability dict yet, intiailize a subdict for it and add its subsequent state.\n",
    "       transition_probability[sent[i][1]] = {}\n",
    "       transition_probability[sent[i][1]][sent[i+1][1]] = 1\n",
    "    else: #if the state (pos) is in the key list of transition probability dict, update the frequency of its subsequent state.\n",
    "      if sent[i+1][1] not in transition_probability[sent[i][1]].keys():\n",
    "         transition_probability[sent[i][1]][sent[i+1][1]] = 1\n",
    "      else:\n",
    "         transition_probability[sent[i][1]][sent[i+1][1]] += 1\n",
    "\n",
    "#********* SOLUTION Q2 *******\n",
    "# next_states_count = defaultdict(int)\n",
    "# for state, next_state_count_dict in transition_probability.items():\n",
    "#     next_states_count[state] += sum(next_state_count_dict.values())\n",
    "\n",
    "# for state, next_state_count_dict in transition_probability.items():\n",
    "#     for next_state, count in next_state_count_dict.items():\n",
    "#         transition_probability[state][next_state] = transition_probability[state][next_state]/next_states_count[state]\n",
    "        \n",
    "# for state, next_state_count_dict in transition_probability.items():\n",
    "#     transition_probability[state][next_state_count_dict] = normalize_and_smoothing(next_state_count_dict, states)\n",
    "#     emission_probability[state][word] = normalize_and_smoothing(word_count_dict, observations)\n",
    "    \n",
    "for state, next_state_count_dict in transition_probability.items():\n",
    "    transition_probability[state] = normalize_and_smoothing(next_state_count_dict, states)\n",
    "        \n",
    "# for state, word_count_dict in transition_probability.items():\n",
    "#     print(sorted(word_count_dict.values()))\n",
    "#     print(\"\\n\")\n",
    "\n",
    "#*******************************\n",
    "\n",
    "\n",
    "start_probability = {} #initialize the transition probability matrix.\n",
    "for sent in dataset: #construct the start probability dictionary which contains the frequency of each state appearing at the start position of a sentence.\n",
    "  if sent[0][1] not in start_probability.keys():\n",
    "     start_probability[sent[0][1]] = 1\n",
    "  else:\n",
    "     start_probability[sent[0][1]] += 1\n",
    "start_probability = normalize_and_smoothing(start_probability, states) #normalize the start probability dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBW-HYEXjfPm"
   },
   "source": [
    "## 5. Implementing the Viterbi Algorithm \n",
    "\n",
    "The code below is implementing the Viterbi algorithm. <br>\n",
    "**Q3**: Please add comments to the lines with \"COMMENT NEEDED\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "FjjacMrG34Hb"
   },
   "outputs": [],
   "source": [
    "def Viterbi(input_observations, states, start_probability, transition_probability, emission_probability):\n",
    "    path = {s:[] for s in states} #this dictionary saves the shortest path for each state.\n",
    "    curr_probability = {}\n",
    "    for s in states:\n",
    "        curr_probability[s] = start_probability[s]*emission_probability[s][input_observations[0]] #\"COMMENT NEEDED\" (Q3)\n",
    "    for i in range(1, len(input_observations)):\n",
    "        last_probability = curr_probability #update the current state probabilities to the ones obtained in the last step.\n",
    "        curr_probability = {} #initialize a new dict to save the state probabilities for this step. \n",
    "        for curr_state in states: #for each possible state in this step, compute all possible paths and their probabilities towards this state.\n",
    "            if input_observations[i] not in emission_probability[curr_state]:\n",
    "                possible_paths = [(last_probability[last_state]*transition_probability[last_state][curr_state]*min(emission_probability[curr_state].values()), last_state) for last_state in states]\n",
    "            else:\n",
    "                possible_paths = [(last_probability[last_state]*transition_probability[last_state][curr_state]*emission_probability[curr_state][input_observations[i]], last_state) for last_state in states]\n",
    "            max_probability, last_state = max(possible_paths) #\"COMMENT NEEDED\" (Q3)\n",
    "            curr_probability[curr_state] = max_probability #update the current state probabilities.\n",
    "            path[curr_state].append(last_state) #record the optimal path towards each possible state in this step.\n",
    "    for s in states:\n",
    "        path[s].append(s) #complete the path with each state as the end.\n",
    "\n",
    "   #\"COMMENT NEEDED\" (Q3)\n",
    "    sorted_probability = sorted(curr_probability.items(), key=lambda x: x[1], reverse=True)\n",
    "    the_last_state = sorted_probability[0][0]\n",
    "    optimal_path = path[the_last_state]\n",
    "\n",
    "    return optimal_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyrz5CKi_QAr"
   },
   "source": [
    "## 6. Applying the the Viterbi Algorithm \n",
    "\n",
    "**Q4:** Apply the Viterbi Algorithm to the following sentences (remember to deal with unknown words): <br>\n",
    "(a) \"there are also many plants in the square.\" <br>\n",
    "(b) \"we will move the table into cs123.\" <br>\n",
    "\n",
    "Please report the tagging results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "z0ESw9woAXzk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RB', 'VBP', 'RB', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION Q4(a)\n",
    "input_a = [\"there\", \"are\", \"also\", \"many\", \"plants\", \"in\", \"the\", \"square\", \".\"]\n",
    "sol_a = Viterbi(input_a, states, start_probability, transition_probability, emission_probability)\n",
    "print(sol_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "PZ0_L4X7Au6k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRP', 'MD', 'VB', 'DT', 'NN', 'NN', 'NNP', '.']\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION Q4(b)\n",
    "input_b = [\"we\", \"will\", \"move\", \"the\", \"tables\", \"into\", \"cs123\", \".\"]\n",
    "sol_b = Viterbi(input_b, states, start_probability, transition_probability, emission_probability)\n",
    "print(sol_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 4 (part 1): Traditional_POS_Tagging_viterbi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
