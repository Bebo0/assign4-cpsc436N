{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyG4lTR4D7Lg"
   },
   "source": [
    "#  Assignment 4 (part 2)(CPSC 436N): LSTM-Based Part-Of-Speech (POS) Tagger\n",
    "\n",
    "In Part2 of this assignment, we are also using the text file called __cmpt-hw2-3.txt__ to train a bidirectional stacked LSTM-based neural sequence labeling model to predict the part of speech tags of unknown input sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFaEQAQgEIew"
   },
   "source": [
    "## 1. import libs and packages needed for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "a4c3BccQvpZ_",
    "outputId": "c603744a-7121-447b-c9b4-b6a95d3c43a4",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Cti1kgGqD9a4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEzE9RjwE0fI"
   },
   "source": [
    "## 2. Load the dataset (cmpt-hw2-3.txt)\n",
    "\n",
    "We will use the same dataset as in (Part1), namely **cmpt-hw2-3.txt**. Please first upload this text file from your local computer to Colab by following these instructions:\n",
    "\n",
    "https://colab.research.google.com/notebooks/io.ipynb\n",
    "\n",
    "**Please note that the uploaded file will be deleted once the connection is terminated. So you need to upload the file everytime you connect to google colab.**\n",
    "\n",
    "<br>\n",
    "\n",
    "For convenience, we will load and save each line of the dataset as **a token list** and **a pos list**. For example, one line in our dataset:\n",
    "\n",
    "*There_EX are_VBP also_RB plant_NN and_CC gift_NN shops_NNS ._.*\n",
    "\n",
    "will be saved as:\n",
    "\n",
    "__token list:__ ['there', 'are', 'also', 'plant', 'and', 'gift', 'shops', '.']\n",
    "\n",
    "__pos list:__ ['EX', 'VBP', 'RB', 'NN', 'CC', 'NN', 'NNS', '.']\n",
    "\n",
    "\n",
    "**Please note that we convert each token into its lower case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "n_ob5y-z7psi"
   },
   "outputs": [],
   "source": [
    "data_path = './cmpt-hw2-3.txt'\n",
    "\n",
    "dataset = [] #initialize dataset list.\n",
    "for line in open(data_path): #load data from the text file.\n",
    "    text_list = []; pos_list = [];\n",
    "    l = line.strip().split(' ') #split each line in the file by space.\n",
    "    for w in l:\n",
    "      w = w.split('_') #each token and its pos tag are connected by an underscore, here we split it by undescore.\n",
    "      text_list.append(w[0].lower()) #add token to the text list for the current line.\n",
    "      pos_list.append(w[1]) #add pos tag to the pos list for the current line.\n",
    "    dataset.append((text_list, pos_list)) #add the processed line (text and pos list) into the dataset list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lps3RqC3FAgA"
   },
   "source": [
    "## 3. Split the loaded dataset into training/dev/testing subsets\n",
    "\n",
    "Here we follow the common scheme to split this dataset into training/dev/test set with ratio 80%-10%-10%. After shuffling the data to avoid any possible ordering bias, we take the first 80% samples as training set, then the next 10% samples as dev set, and the last 10% samples as testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "R6Mdvr5WCfZt"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "training_data = dataset[0:math.floor(len(dataset)*0.8)]\n",
    "dev_data = dataset[math.floor(len(dataset)*0.8):math.floor(len(dataset)*0.9)]\n",
    "test_data = dataset[math.floor(len(dataset)*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WzNS6VagmvT"
   },
   "source": [
    "## 4. Construct word-to-index and tag-to-index mapping dictionary\n",
    "\n",
    "We model POS tagging  as a sequence labeling task. The pipeline can be described as:\n",
    "\n",
    "<br>\n",
    "\n",
    "*a sequence of tokens -(mapping)-> a sequence of token indices --(input)--> POS Tagger --(output)--> a sequence of POS tag indices --(mapping)--> a sequence of POS tags*\n",
    "\n",
    "<br>\n",
    "\n",
    "So in this step, we want to construct two mapping dictionaries to assign a unique index to each token and tag respectively (see code below). These two mapping dictionaries will be used in the first and last mapping steps in the above pipeline.\n",
    "\n",
    "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "As in Part1, we also need to deal with **unknown tokens** in testing. The code below is implementing a possible way to adderess this problem. ***Q5: Please add comments to the lines with \"COMMENT NEEDED\". And describe in your words the implemented solution.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Wg0oO1B4OlsD"
   },
   "outputs": [],
   "source": [
    "class Word_Dictionary(object): #this class is used to generate and return the word-to-index (index-to-word) vocabulary dictionary.\n",
    "    def __init__(self): #initializing.\n",
    "        self.word2idx = {'_unk_':0} # This code is a place holder for later handling unknown tokens\n",
    "        self.idx2word = {0:'_unk_'} # This code associates the '_unk_' key with index zero \n",
    "                                    # to allow us to use it as a threshold\n",
    "        self.idx = 1\n",
    "    \n",
    "    def add_word(self, word): #add a new word into the dictionary and assign an unique index to it.\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "        \n",
    "    def __len__(self): #return the number of words in this dictionary.\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Tag_Dictionary(object): #this class is used to generate and return the tag-to-index (index-to-tag) dictionary.\n",
    "    def __init__(self): #initializing.\n",
    "        self.tag2idx = {}\n",
    "        self.idx2tag = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_tag(self, tag): #add a new tag into the dictionary and assign an unique index to it.\n",
    "        if not tag in self.tag2idx:\n",
    "            self.tag2idx[tag] = self.idx\n",
    "            self.idx2tag[self.idx] = tag\n",
    "            self.idx += 1\n",
    "        \n",
    "    def __len__(self): #return the number of tags in this dictionary.\n",
    "        return len(self.tag2idx)\n",
    "\n",
    "\n",
    "wd_dict = Word_Dictionary() #initialize the word-to-index dictionary.\n",
    "tag_dict = Tag_Dictionary() #initialize the tag-to-index dictionary.\n",
    "unknown_threshold = 1 # Since we used zero as the index for the '_unk_' token placeholder, \n",
    "                      # we need to start above zero when mapping known words to our word dictionary\n",
    "\n",
    "word_count = {} #initialize the dictionary to save frequencies of words.\n",
    "\n",
    "for sample in training_data: #fill the word frequency dictionary with training data.\n",
    "    text, tags = sample\n",
    "    for word in text:\n",
    "      if word not in word_count.keys():\n",
    "        word_count[word] = 1\n",
    "      else:\n",
    "        word_count[word] += 1\n",
    "\n",
    "for sample in training_data: #fill the word-to-index and tag-to-index dictionary with training data.\n",
    "    text, tags = sample\n",
    "    for word in text:\n",
    "      if word_count[word] > unknown_threshold:  # If the count of the current token is above one, add word to dict\n",
    "                                                # Essentially, we use this threshold to check word frequency \n",
    "                                                # and ensure all words appear at least twice, \n",
    "                                                # filtering out words that only appear once\n",
    "                                                # If they only appear once, the token will later be replaced with 'unknown'\n",
    "                                                # during the data processing code below\n",
    "        wd_dict.add_word(word)\n",
    "    for tag in tags:\n",
    "      tag_dict.add_tag(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8K7BDZ-rkra"
   },
   "source": [
    "## 5. Implement the function for data processing\n",
    "\n",
    "In this step, we implement a function processing our data into the format which is ready for neural POS tagger's training and testing.\n",
    "\n",
    "More specifically, we convert the input token and pos tag list into tensors (i.e., vectors) containing their corresponding indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6NYGuy5WCiHh"
   },
   "outputs": [],
   "source": [
    "def data_processing_for_lstm(sample, word_dict, tag_dict, unknown_threshold): #this function convert a sample into the format ready for our neural POS tagger.\n",
    "    text, tags = sample\n",
    "    word_ids = []; tag_ids = []\n",
    "\n",
    "    for word in text:\n",
    "      if word in word_dict.word2idx.keys() and word_count[word] > unknown_threshold: #map the token to its index if its frequency is over a threshold.\n",
    "        word_ids.append(word_dict.word2idx[word])\n",
    "      else: #map the token to the index of unknown token if its frequency is not over a threshold.\n",
    "        word_ids.append(word_dict.word2idx['_unk_'])\n",
    "\n",
    "    for tag in tags: #map pos tags into indices using the tag-to-index dictionary.\n",
    "      tag_ids.append(tag_dict.tag2idx[tag])\n",
    "\n",
    "    word_ids = torch.from_numpy(np.array(word_ids)) #convert the list of token ids into tensor format.\n",
    "    tag_ids = torch.from_numpy(np.array(tag_ids)) #convert the list of tag ids into tensor format.\n",
    "\n",
    "    return word_ids, tag_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maHAotm7rtTg"
   },
   "source": [
    "## 6. The class of LSTM-based POS tagger\n",
    "\n",
    "Now let's design the architecture of our bidirectional stacked LSTM-based POS tagger. It should consist of three layers:\n",
    "\n",
    "* [Embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html): projecting input token ids into its embedding space.\n",
    "* [(Bi-)LSTM hidden state layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
    "* [Output layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html): converting the hidden states to POS predictions.\n",
    "\n",
    "**Q6:** Please after carefully reading the pytorch links above, ***add comments to the lines with \"COMMENT NEEDED\".***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DP60pf4ByqPs"
   },
   "outputs": [],
   "source": [
    "def zero_state(module, batch_size, device): #the function to initialize the states of the bidirectionsl LSTM.\n",
    "    # * 2 is for the two directions\n",
    "    return Variable(torch.zeros(module.num_layers * 2, batch_size, module.hidden)).to(device), \\\n",
    "           Variable(torch.zeros(module.num_layers * 2, batch_size, module.hidden)).to(device)\n",
    "\n",
    "class LSTM_tagger(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden, num_layers, tag_size, device):\n",
    "        super(LSTM_tagger, self).__init__()\n",
    "        self.device = device #the device we run our model on.\n",
    "        self.num_layers = num_layers # Number of recurrent layers. \n",
    "                                     # since we want a stacking LSTM, we set it to the desired number of layers\n",
    "                                     # this means every LSTM following the initial one will take in \n",
    "                                     # outputs of the first LSTM to compute the final results.\n",
    "\n",
    "        self.hidden = hidden #the dimension of hidden state.\n",
    "        self.input_size = embed_size # the size of each embedding vector\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size) #the word embedding layer (convert words to embeddings).\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
    "                            hidden_size=self.hidden,\n",
    "                            num_layers=self.num_layers,\n",
    "                            dropout=0.5, # This value sets the dropout probability, \n",
    "                                       # it will be equal to 1 with probability p and 0 otherwise (Bernoulli(p))\n",
    "                                       # so here we set it to zero as default, which means not introducing any dropout layers\n",
    "                            \n",
    "\n",
    "                            bidirectional=True) # Sets LSTM to be bidirectional so that we can do back propagation\n",
    "\n",
    "        self.h2s = nn.Linear(hidden * 2, tag_size) #the linear output layer (convert from hidden states of LSTM to the list of tag scores).\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) #convert word list to embedding list for the input sample.\n",
    "        x = x.view(len(x),1,-1)\n",
    "        s = zero_state(self, 1, self.device) #initialize the states of LSTM.\n",
    "        lstm_output, _ = self.lstm(x, s) #LSTM.\n",
    "        outputs = self.h2s(lstm_output.view(len(x),-1)) # This is the linearly transformed output of the hidden states of the LSTM\n",
    "                                                        # where all but the last dimension are the same shape as the input \n",
    "                                                        # producing a list of tag scores (?)\n",
    "\n",
    "        tags_probs = F.log_softmax(outputs,dim=1) # Since the outputs will be a list of scores that come from summing the weighted features and bias,\n",
    "                                                  # the output could be any real number and not a probability distribution\n",
    "                                                  # so we use log softmax to normalize the values and give us a distribution\n",
    "\n",
    "        return tags_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQwufkRpr432"
   },
   "source": [
    "## 7. Initialize model and define parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1zpe1I2clIkV"
   },
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "intermediate_size = 256\n",
    "num_layers = 2\n",
    "vocab_size = wd_dict.__len__()\n",
    "tag_size = tag_dict.__len__()\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.002\n",
    "device = 0 if torch.cuda.is_available() else 'cpu' # Specify the device (CPU/GPU) to train/eval the model on\n",
    "criterion = nn.CrossEntropyLoss() # We need loss to compute the updates to the weights\n",
    "\n",
    "model = LSTM_tagger(vocab_size, embed_size, intermediate_size, num_layers, tag_size, device)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # The optimizer holds the current state \n",
    "                                                                   # and will update the parameters based on the computed gradients. \n",
    "                                                                   # We need this to update the weights to improve accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN2ZI8wEr90x"
   },
   "source": [
    "## 8. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KLkBijlBlJTg",
    "outputId": "1462161b-a1a5-44df-9146-2ce6f97b28e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1776/1776 [00:33<00:00, 52.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0566, F1 score:  0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1776/1776 [00:32<00:00, 54.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.0562, F1 score:  0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1776/1776 [00:32<00:00, 54.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.0578, F1 score:  0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1776/1776 [00:32<00:00, 55.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.0577, F1 score:  0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1776/1776 [00:31<00:00, 55.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.0601, F1 score:  0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1776/1776 [00:31<00:00, 55.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.0616, F1 score:  0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1776/1776 [00:31<00:00, 55.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.0564, F1 score:  0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1776/1776 [00:31<00:00, 56.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.0625, F1 score:  0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1776/1776 [00:31<00:00, 56.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.0598, F1 score:  0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1776/1776 [00:31<00:00, 56.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.0634, F1 score:  0.93\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    avg_ppl = 0; avg_loss = 0;\n",
    "    for i in tqdm(range(0, len(training_data))): #input one sample a time to train the model.\n",
    "        inputs, targets = data_processing_for_lstm(training_data[i], wd_dict, tag_dict, unknown_threshold)  #prepare the sample for model training.\n",
    "        \n",
    "        #forward pass.\n",
    "        outputs = model(inputs.to(device)) # Returns the probability distribution for the tags\n",
    "        loss = criterion(outputs, targets.reshape(-1).to(device)) # give the crossEntropyLoss the weights and the size index \n",
    "                                                                  # to compute the loss between input and target at each step \n",
    "\n",
    "        avg_loss += loss.item() # Accumulate the total loss to average it later  \n",
    "        \n",
    "        #backward and optimize.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "    \n",
    "    #validation step.\n",
    "    preds = []; targets = [];\n",
    "    for i in range(0, len(dev_data)):\n",
    "        dev_inputs, dev_targets = data_processing_for_lstm(dev_data[i], wd_dict, tag_dict, unknown_threshold)\n",
    "        dev_outputs = model(dev_inputs.to(device))\n",
    "        dev_pred = torch.argmax(dev_outputs, dim=-1)\n",
    "        preds += dev_pred.detach().cpu().numpy().tolist()\n",
    "        targets += dev_targets.tolist()\n",
    "    \n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}, F1 score: {:5.2f}'\n",
    "        .format(epoch+1, num_epochs, avg_loss/len(training_data), f1_score(targets, preds, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXCXOQx9sFb-"
   },
   "source": [
    "## 9. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGBnD9cuSxgT",
    "outputId": "2605eff1-ac8a-43b7-8c79-6666fcb034b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [00:01<00:00, 127.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.881153838569011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_preds = []; test_targets = [];\n",
    "for i in tqdm(range(0, len(test_data))):\n",
    "    test_input, test_target = data_processing_for_lstm(test_data[i], wd_dict, tag_dict, unknown_threshold)\n",
    "    test_output = model(test_input.to(device))\n",
    "    test_pred = torch.argmax(test_output, dim=-1)\n",
    "    test_preds += test_pred.detach().cpu().numpy().tolist()\n",
    "    test_targets += test_target.tolist()\n",
    "print(f1_score(test_targets, test_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37a9aRkBaKvq"
   },
   "source": [
    "**Q7:** Now train and test a different model with a dropout rate of 0.05. Report the performance and a plausible explanation for the different performance of the two tested models (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8TjMFyxOhq4"
   },
   "source": [
    "\n",
    "\n",
    "Where d = 0\n",
    "\n",
    "Performance: 0.9050487484286546\n",
    "\n",
    "Where d = 0.5\n",
    "\n",
    "Performance: 0.8930199021777645 \n",
    "\n",
    "Performance after retraining without changing hyperparams: 0.881153838569011\n",
    "\n",
    "The goal of adding dropout is to improve performance by preventing overfitting. Hence, I was expecting the performance with d = 0.5 to be better because the regularization parameter, p(1-p), is maximum at p = 0.5. \n",
    "\n",
    "However, the results here to not align with the principle of dropout. Perhaps this is a product of what is being randomly dropped, or perhaps the dataset is so small that dropping some units from the network during training is not helpful.\n",
    "\n",
    "That said, the difference in performance is not extreme, at just over 0.01~2%, so the dropout value may not matter much anyway.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_4_(part_2)_Neural_POS_Tagging_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyCharm (assign4-cpsc436N)",
   "language": "python",
   "name": "pycharm-e01fd618"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}